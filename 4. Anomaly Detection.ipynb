{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection (in non sequential data).\n",
    "\n",
    "Anomaly detection algorithms detect observations that are significantly different from most of what you've seen before.\n",
    "\n",
    "One classic example here is in detecting credit card fraud: how do we automatically detect purchases that a legitimate credit card owner is very unlikely to have made?\n",
    "\n",
    "Another is in systems security: how do we detect activity on a network that's unlikely to be caused be a legitimate user?\n",
    "\n",
    "The simplest possible case for anomaly detection is observational data with a single, normally distributed feature. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a data sample from a normal pdf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "plotBlue = sns.color_palette()[0]\n",
    "np.random.seed(3)\n",
    "\n",
    "N = 1000\n",
    "X1 = np.random.normal(4, 12, N)\n",
    "f, axes = plt.subplots(nrows=2, sharex=True)\n",
    "axes[0].set_xlim(-50, 50)\n",
    "axes[0].scatter(X1, np.zeros(N), marker='x', c=plotBlue)\n",
    "axes[1].hist(X1, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To model this data as a normal distribution, we compute the mean and the standard deviation from the sample we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_mean = X1.mean()\n",
    "sample_sigma = X1.std()\n",
    "print('Sample Mean:', sample_mean)\n",
    "print('Sample Standard Deviation:', sample_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just have to decide on some $\\epsilon$ value, which dictates our probability threshold for anomalous events. \n",
    "\n",
    "If we set $\\epsilon$ to .01, we're saying that any draw for which there's a probability of 1% or less that it given the normal distribution should be marked as anomalous. \n",
    "\n",
    "In general, if a sample follows the normal model with mean $\\mu$ and standard error $\\sigma$, then the\n",
    "**confidence interval** of its samples is:\n",
    "\n",
    "$$ \\mu \\pm z \\times \\sigma $$\n",
    "\n",
    "where $z$ corresponds to the confidence level selected:\n",
    "\n",
    "| Confidence Level  | z Value  | \n",
    "|---|---|\n",
    "|  90% | 1.65 |\n",
    "|  95% | 1.96 |\n",
    "|  99% | 2.58 |\n",
    "|  99,9% | 3.291 |\n",
    "\n",
    "\n",
    "These values are the upper and lower bounds for what we consider 'normal', and are represented in the graphs below by the area shaded in red. Our estimate for the distribution therefore looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base = np.linspace(-50, 50, 100)\n",
    "normal = sp.stats.norm.pdf(base, sample_mean, sample_sigma)\n",
    "\n",
    "lower_bound = sample_mean - (2.58 * sample_sigma)\n",
    "upper_bound = sample_mean + (2.58 * sample_sigma)\n",
    "anomalous = np.logical_or(base < [lower_bound]*100, base > [upper_bound]*100)\n",
    "\n",
    "plt.plot(base, normal)\n",
    "plt.fill_between(base, normal, where=anomalous, color=[1, 0, 0, 0.4])\n",
    "plt.xlim(-50, 50)\n",
    "plt.show()\n",
    "print('Lower Bound:', lower_bound)\n",
    "print('Upper Bound:', upper_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at two sample draws to see if they're anomalous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X1, np.zeros(N), marker='x', c=plotBlue)\n",
    "plt.xlim(-50, 50)\n",
    "plt.scatter(-29, 0, marker='x', color='red', s=150, linewidths=3)\n",
    "plt.scatter(17, 0, marker='x', color='green', s=150, linewidths=3)\n",
    "plt.axvline(lower_bound, ymin=.25, ymax=.75, color='red', linewidth=1)\n",
    "plt.axvline(upper_bound, ymin=.25, ymax=.75, color='red', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now expand our analysis to multiple variables. Initially we will assume that they are **independently** normal distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "X1 = np.random.normal(4, 12, N)\n",
    "X2 = np.random.normal(9, 5, N)\n",
    "plt.scatter(X1, X2, c=plotBlue, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can estimate the means and standard deviations of the normal distributions through the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1_sample_mean = X1.mean()\n",
    "x2_sample_mean = X2.mean()\n",
    "x1_sample_sigma = X1.std()\n",
    "x2_sample_sigma = X2.std()\n",
    "print('Sample Mean 1:', x1_sample_mean)\n",
    "print('Sample Mean 2:', x2_sample_mean)\n",
    "print('Sample Standard Deviation 1:', x1_sample_sigma)\n",
    "print('Sample Standard Deviation 2:', x2_sample_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we would expect, these are not far from the actual values we used to generate the data.\n",
    "\n",
    "Next, let's look at a heatmap of where we would expect to find observations given the joint probability distributions implied by these distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "delta = 0.025\n",
    "x1 = np.arange(-60, 60, delta)\n",
    "x2 = np.arange(-15, 30, delta)\n",
    "x, y = np.meshgrid(x1, x2)\n",
    "\n",
    "z = plt.mlab.bivariate_normal(x, y, x1_sample_sigma, x2_sample_sigma, x1_sample_mean, x2_sample_mean)\n",
    "plt.contourf(x, y, z, cmap='bwr')\n",
    "\n",
    "thinned_points = np.array([n in np.random.choice(N, 300) for n in range(N)])\n",
    "plt.scatter(X1[thinned_points], X2[thinned_points], c='yellow', alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we move in towards the means, we're increasingly likely to draw an observation with those features. As we move away, we're less likely to see an observation with features at those values. We might, for instance, decide that anything in the dark-blue region is anomalous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need a way to calculate the probability that a data point belongs to a normal distribution given some set of parameters. Fortunately SciPy has this built-in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import stats  \n",
    "\n",
    "X=[a for a in zip(X1,X2)]\n",
    "\n",
    "X=np.array(X)\n",
    "dist = stats.norm(x1_sample_mean, x1_sample_sigma)  \n",
    "dist.pdf(X[:,0])[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just calculated the probability that each of the first 50 instances of our data set's first dimension belong to the distribution that we defined earlier by calculating the mean and variance for that dimension. \n",
    "\n",
    "Essentially it's computing how far each instance is from the mean and how that compares to the \"typical\" distance from the mean for this data.\n",
    "\n",
    "Let's compute and save the probability density of each of the values in our data set given the Gaussian model parameters we calculated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = np.zeros((X.shape[0], X.shape[1]))  \n",
    "p[:,0] = stats.norm(x1_sample_mean, x1_sample_sigma).pdf(X[:,0])  \n",
    "p[:,1] = stats.norm(x2_sample_mean, x2_sample_sigma).pdf(X[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outliers = np.where(p < 0.0009)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4))  \n",
    "ax.scatter(X[:,0], X[:,1], alpha=0.4)  \n",
    "ax.scatter(X[outliers[0],0], X[outliers[0],1], s=30, color='r', marker='o')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The threshold value can be selected by using the $F_1$ score on a cross-validation set where true anomalies should be manually labeled.\n",
    "\n",
    "The $F_1$ score is a measure of a test's accuracy. It considers both the precision $p$ and the recall $r$ of the test:\n",
    "\n",
    "$$F1 = 2 * \\frac{(precision * recall)}{(precision + recall)}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$ p = \\frac{tp}{tp+fp} $$\n",
    "\n",
    "$$ r = \\frac{tp}{tp+fn} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def select_threshold(pval, yval):  \n",
    "    best_epsilon = 0\n",
    "    best_f1 = 0\n",
    "    f1 = 0\n",
    "\n",
    "    step = (pval.max() - pval.min()) / 1000\n",
    "\n",
    "    for epsilon in np.arange(pval.min(), pval.max(), step):\n",
    "        preds = pval < epsilon\n",
    "\n",
    "        tp = np.sum(np.logical_and(preds == 1, yval == 1)).astype(float)\n",
    "        fp = np.sum(np.logical_and(preds == 1, yval == 0)).astype(float)\n",
    "        fn = np.sum(np.logical_and(preds == 0, yval == 1)).astype(float)\n",
    "\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_epsilon = epsilon\n",
    "\n",
    "    return best_epsilon, best_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "In this exercise, you will implement the anomaly detection algorithm and apply it to detect failing servers on a network.\n",
    "\n",
    "The features measure the throughput (mb/s) and latency (ms) of response of each server. While your servers were operating, you collected $m = 307$ examples of how they were behaving, and thus have an unlabeled dataset $(x_1, \\dots, x_m)$.\n",
    "\n",
    "You suspect that the vast majority of these examples are “normal” (non-anomalous) examples of the servers operating normally, but there might also be some examples of servers acting anomalously within this dataset.\n",
    "\n",
    "You will use a Gaussian model to detect anomalous examples in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(context=\"notebook\", style=\"white\", palette=sns.color_palette(\"RdBu\"))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Read the cvs file ``files/ex8data1.csv``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Visualize the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Let's suppose independent features. Create a simple function that calculates the mean and variance for each feature in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Find (manually) a threshold value for detcting outliers. Visualize the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Methods: One-class SVM with non-linear kernel (RBF)\n",
    "\n",
    "One-class SVM is an unsupervised algorithm that learns a decision function for novelty detection: classifying new data as similar or different to the training set.\n",
    "\n",
    "nu : An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "from sklearn import svm\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(-5, 5, 500), np.linspace(-5, 5, 500))\n",
    "# Generate train data\n",
    "X = 0.3 * np.random.randn(100, 2)\n",
    "X_train = np.r_[X + 2, X - 2]\n",
    "# Generate some regular novel observations\n",
    "X = 0.3 * np.random.randn(20, 2)\n",
    "X_test = np.r_[X + 2, X - 2]\n",
    "# Generate some abnormal novel observations\n",
    "X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))\n",
    "\n",
    "# fit the model\n",
    "clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n",
    "clf.fit(X_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "y_pred_outliers = clf.predict(X_outliers)\n",
    "n_error_train = y_pred_train[y_pred_train == -1].size\n",
    "n_error_test = y_pred_test[y_pred_test == -1].size\n",
    "n_error_outliers = y_pred_outliers[y_pred_outliers == 1].size\n",
    "\n",
    "# plot the line, the points, and the nearest vectors to the plane\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.title(\"Novelty Detection\")\n",
    "plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu)\n",
    "a = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkred')\n",
    "plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='palevioletred')\n",
    "\n",
    "s = 40\n",
    "b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white', s=s)\n",
    "b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='blueviolet', s=s)\n",
    "c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='gold', s=s)\n",
    "plt.axis('tight')\n",
    "plt.xlim((-5, 5))\n",
    "plt.ylim((-5, 5))\n",
    "plt.legend([a.collections[0], b1, b2, c],\n",
    "           [\"learned frontier\", \"training observations\",\n",
    "            \"new regular observations\", \"new abnormal observations\"],\n",
    "           loc=\"upper left\",\n",
    "           prop=matplotlib.font_manager.FontProperties(size=11))\n",
    "plt.xlabel(\n",
    "    \"error train: %d/200 ; errors novel regular: %d/40 ; \"\n",
    "    \"errors novel abnormal: %d/40\"\n",
    "    % (n_error_train, n_error_test, n_error_outliers))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Methods: Robust linear model estimation using RANSAC.\n",
    "\n",
    "Random sample consensus (RANSAC) is an iterative method to estimate parameters of a mathematical model from a set of observed data that contains outliers, when outliers are to be accorded no influence on the values of the estimates. Therefore, it also can be interpreted as an outlier detection method.\n",
    "\n",
    "The input to the RANSAC algorithm is a set of observed data values, a way of fitting some kind of model to the observations, and some confidence parameters. RANSAC achieves its goal by repeating the following steps:\n",
    "\n",
    "+ Select a random subset of the original data. Call this subset the hypothetical inliers.\n",
    "+ A model is fitted to the set of hypothetical inliers.\n",
    "+ All other data are then tested against the fitted model. Those points that fit the estimated model well, according to some model-specific loss function, are considered as part of the consensus set.\n",
    "\n",
    "The estimated model is reasonably good if sufficiently many points have been classified as part of the consensus set.\n",
    "Afterwards, the model may be improved by reestimating it using all members of the consensus set.\n",
    "\n",
    "This procedure is repeated a fixed number of times, each time producing either a model which is rejected because too few points are part of the consensus set, or a refined model together with a corresponding consensus set size. In the latter case, we keep the refined model if its consensus set is larger than the previously saved model.\n",
    "\n",
    "From Wikipedia [https://en.wikipedia.org/wiki/Random_sample_consensus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import linear_model, datasets\n",
    "\n",
    "\n",
    "n_samples = 1000\n",
    "n_outliers = 50\n",
    "\n",
    "\n",
    "X, y, coef = datasets.make_regression(n_samples=n_samples, n_features=1,\n",
    "                                      n_informative=1, noise=10,\n",
    "                                      coef=True, random_state=0)\n",
    "\n",
    "# Add outlier data\n",
    "np.random.seed(0)\n",
    "X[:n_outliers] = 3 + 0.5 * np.random.normal(size=(n_outliers, 1))\n",
    "y[:n_outliers] = -3 + 10 * np.random.normal(size=n_outliers)\n",
    "\n",
    "# Fit line using all data\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Robustly fit linear model with RANSAC algorithm\n",
    "model_ransac = linear_model.RANSACRegressor(linear_model.LinearRegression())\n",
    "model_ransac.fit(X, y)\n",
    "inlier_mask = model_ransac.inlier_mask_\n",
    "outlier_mask = np.logical_not(inlier_mask)\n",
    "\n",
    "# Predict data of estimated models\n",
    "line_X = np.arange(-5, 5)\n",
    "line_y = model.predict(line_X[:, np.newaxis])\n",
    "line_y_ransac = model_ransac.predict(line_X[:, np.newaxis])\n",
    "\n",
    "# Compare estimated coefficients\n",
    "print(\"Estimated coefficients (true, normal, RANSAC):\")\n",
    "print(coef, model.coef_, model_ransac.estimator_.coef_)\n",
    "\n",
    "lw = 2\n",
    "plt.scatter(X[inlier_mask], y[inlier_mask], color='yellowgreen', marker='.',\n",
    "            label='Inliers')\n",
    "plt.scatter(X[outlier_mask], y[outlier_mask], color='gold', marker='.',\n",
    "            label='Outliers')\n",
    "plt.plot(line_X, line_y, color='navy', linestyle='-', linewidth=lw,\n",
    "         label='Linear regressor')\n",
    "plt.plot(line_X, line_y_ransac, color='cornflowerblue', linestyle='-',\n",
    "         linewidth=lw, label='RANSAC regressor')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection (in time series).\n",
    "\n",
    "People spend a lot of time watching the data.\n",
    "\n",
    "### The problem\n",
    "\n",
    "It’s hard to manually spot when something has changed enough to care about. If something has changed, it’s hard to identify why.\n",
    "\n",
    "### The solution\n",
    "\n",
    "Implement a system which watches for unexpected changes. When a change occurs, offer likely explanations for the change so people can investigate.\n",
    "\n",
    "### Examples\n",
    "\n",
    "Let's suppose we are an online retailer. The business metrics we are interested in are:\n",
    "\n",
    "+ Number of orders\n",
    "+ Total value of orders\n",
    "+ Average value of order\n",
    "+ Number of web visits\n",
    "+ Order rate of web visits\n",
    "+ Bounce rate\n",
    "\n",
    "And the possible origins of change are:\n",
    "\n",
    "+ Country\n",
    "+ Device type (e.g. mobile)\n",
    "+ Web landing page\n",
    "+ Web traffic source\n",
    "+ Retailer\n",
    "\n",
    "Expected explanations:\n",
    "\n",
    "+ One retailer forgets to send us order data. > The number of orders that we make is lower than expected.\n",
    "+ Checkout is broken for mobile web. > The number of orders that we make is lower than expected.\n",
    "+ One retailer is having a sale. > The average value of an order falls.\n",
    "+ It’s sale season in a particular country. > The average value of an order falls.\n",
    "+ A common landing point of our website is broken. > Bounce rate increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many types of signals, and \"change\" can mean very different things.\n",
    "\n",
    "**Broken trend or seasonality**: These are monthly-average daily calls to directory assistance Jan. 62 – Dec 76. We see a sudden drop in activity in this signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "signal = [350,339,351,364,369,331,331,340,346,341,357,398,381,367,383,375,353,361,375, \\\n",
    "          371,373,366,382,429,406,403,429,425,427,409,402,409,419,404,429,463,428,449, \\\n",
    "          444,467,474,463,432,453,462,456,474,514,489,475,492,525,527,533,527,522,526, \\\n",
    "          513,564,599,572,587,599,601,611,620,579,582,592,581,630,663,638,631,645,682, \\\n",
    "          601,595,521,521,516,496,538,575,537,534,542,538,547,540,526,548,555,545,594, \\\n",
    "          643,625,616,640,625,637,634,621,641,654,649,662,699,672,704,700,711,715,718, \\\n",
    "          652,664,695,704,733,772,716,712,732,755,761,748,748,750,744,731,782,810,777, \\\n",
    "          816,840,868,872,811,810,762,634,626,649,697,657,549,162,177,175,162,161,165, \\\n",
    "          170,172,178,186,178,178,189,205,202,185,193,200,196,204,206,227,225,217,219, \\\n",
    "          236,253,213,205,210,216,218,235,241]\n",
    "plt.plot(signal, alpha = 0.5)\n",
    "plt.title('Example: Directory Assistance Calls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abberation in periodicity**: This is a fairly regular sinusoidal signal, shown with an anomaly where one of the waves is squashed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "signal = np.sin(np.linspace(0, 15*np.pi, num=300))\n",
    "signal[105:155] *= 0.1\n",
    "signal = 10 * signal + 50\n",
    "\n",
    "noise = np.random.normal(scale = 1.5, size=300)\n",
    "signal = signal + noise\n",
    "\n",
    "plt.ylim(0,100)\n",
    "plt.plot(signal)\n",
    "plt.title('Example: periodic signal anomaly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Mean Detector\n",
    "\n",
    "We'll start with as basic as a signal that we can consider. It consists of a signal value, repeated, that, at some point, changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sig1 = np.ones(150)\n",
    "sig1[:100] *= 50\n",
    "sig1[100:] *= 40\n",
    "\n",
    "# Size of change in our test signal\n",
    "jump_size = sig1[0] - sig1[-1]\n",
    "\n",
    "# We'll add a small amount (0.02 x jump_size) of Gaussian noise to the signal. \n",
    "noise = np.random.normal(\n",
    "    size=sig1.shape,\n",
    "    scale=jump_size * 0.1)\n",
    "\n",
    "sig1 = sig1 + noise\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(sig1, 'b.')\n",
    "plt.plot(sig1, '-', alpha=0.2)\n",
    "plt.ylim(0,100)\n",
    "plt.title(\"sig1 : A trivial signal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest detector calculates the mean of the signal at each step, and uses stopping rules based on if an incoming signal value differs from the mean by some threshold percent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ser = pd.Series(sig1)\n",
    "mean1 = ser.rolling(window=3,center=False).mean()\n",
    "mean2 = ser.rolling(window=20,center=False).mean()\n",
    "mean_dif = abs(mean1 - mean2)\n",
    "change = mean_dif.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(sig1, 'b.', alpha=0.5)\n",
    "plt.plot(mean1, '-')\n",
    "plt.plot(mean2, '-')\n",
    "plt.plot(mean_dif*5, '-')\n",
    "plt.axvline(x=change, ymin=0, ymax = 100, linewidth=2, color='k')\n",
    "plt.ylim(0,100)\n",
    "plt.title(\"sig1 : A trivial signal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This change detection method has additional weaknesses:\n",
    "\n",
    "+ Sensitive to the threshold value, which we are determining manually.\n",
    "+ Sensitive to anomalous values and outliers\n",
    "+ Signal must be constant. the detector doesn't work well with drift (trend) or local variation (seasonality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a seasonal signal\n",
    "# I imagined a metric that rises from 0 to 5 each calendar month\n",
    "\n",
    "sig2 = np.linspace(0, 5, num=30)\n",
    "sig2 = np.concatenate([sig2 for x in xrange(12)])\n",
    "\n",
    "# Add a jump\n",
    "jump_size = 5\n",
    "sig2[250:] = sig2[250:] + jump_size\n",
    "\n",
    "# Noise\n",
    "noise = np.random.normal(\n",
    "    size=sig2.shape,\n",
    "    scale=jump_size * 0.1)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(sig2 + noise, 'b.', linestyle='')\n",
    "plt.plot(sig2 + noise, 'b-', alpha=0.15)\n",
    "plt.ylim(0,15)\n",
    "plt.xlim(0,365)\n",
    "plt.title(\"Imaginary Seasonal signal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ser = pd.Series(sig2)+noise\n",
    "mean1 = ser.rolling(window=3,center=False).mean()\n",
    "mean2 = ser.rolling(window=20,center=False).mean()\n",
    "mean_dif = abs(mean1 - mean2)\n",
    "change = mean_dif.argmax()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(ser, 'b.', alpha=0.5)\n",
    "plt.plot(mean1, '-')\n",
    "plt.plot(mean2, '-')\n",
    "plt.plot(mean_dif, '-')\n",
    "plt.axvline(x=change, ymin=0, ymax = 14, linewidth=2, color='k')\n",
    "plt.ylim(0,15)\n",
    "plt.xlim(0,365)\n",
    "plt.title(\"sig2 : A non-trivial signal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Write a method to eliminate seasonality and apply a static mean detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an anomaly? (v1.0)\n",
    "\n",
    "An anomaly is a point which deviates from our expectation by a significant margin.\n",
    "\n",
    "> Statistics to the rescue: So we can totally just use $[\\mu \\pm 2\\sigma]$, right? \n",
    "\n",
    "No! Things change over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import linear_model, datasets\n",
    "% matplotlib inline\n",
    "\n",
    "n_samples = 100\n",
    "X, y, coef = datasets.make_regression(n_samples=n_samples, n_features=1,\n",
    "                                      n_informative=1, noise=5,\n",
    "                                      coef=True, random_state=0)\n",
    "\n",
    "X[50]= [-0.90729836]\n",
    "y[50]= -75.759\n",
    "\n",
    "plt.plot([[0] for i in range(n_samples)], y, '.g', label='Inliers')\n",
    "plt.plot([0], y[50], '.r', label='Inliers')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.plot(X, y, '.g', label='Inliers')\n",
    "plt.plot(X[50], y[50], '.r', label='Inliers')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recent data should carry more weight. \n",
    "\n",
    "## What is an anomaly? (v2.0)\n",
    "\n",
    "An anomaly is a point which deviates from our **expectation** by a **significant** margin.\n",
    "\n",
    "Our expectation should be more dependent on the recent past than the whole history.\n",
    "\n",
    "Our definition of significant should be more dependent on the recent past than the whole history. \n",
    "\n",
    "### Idea\n",
    "\n",
    "Predict the future by exponentially weighted mean of the past. This takes all the past into account, but weights the most recent past as more predictive. This is called Holt method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine a weighted average where we consider all of the data points, while assigning exponentially smaller weights as we go back in time. For example if we started with 0.9, our weights would be (going back in time):\n",
    "\n",
    "$$ 0.9^1, 0.9^2, 0.9^3 ... $$\n",
    "\n",
    "Or\n",
    "\n",
    "$$ 0.9, 0.81, 0.729, ... $$\n",
    "\n",
    "…eventually approaching zero. The smaller the starting weight, the faster it approaches zero.\n",
    "\n",
    "Only… there is a problem: weights do not add up to 1. The sum of the first 3 numbers alone is already 2.439! \n",
    "\n",
    "What Holts assures a permanent place in the history of Mathematics is solving this with a succinct and elegant formula:\n",
    "\n",
    "$$ \\hat{y}_x = \\alpha \\cdot y_x + (1 - \\alpha) \\cdot \\hat{y}_{x-1}  $$\n",
    "\n",
    "You can think of $\\alpha$ as a sort of a starting weight 0.9 in the above  example. It is called the smoothing factor or smoothing coefficient. \n",
    "\n",
    "$\\alpha$  is a value that dictates how much weight we give the most recent observed value versus the last expected. It’s a kind of a lever that gives more weight to the left side when it’s higher (closer to 1) or the right side when it’s lower (closer to 0): the higher the $\\alpha$, the faster the method “forgets”.\n",
    "\n",
    "Observation:\n",
    "\n",
    "$$ \\begin{eqnarray}\n",
    "\\hat{y}_x & = & \\alpha \\cdot y_x + (1 - \\alpha) \\cdot \\hat{y}_{x-1} \\\\\n",
    "          & = & \\alpha \\cdot y_x + \\alpha \\cdot(1 - \\alpha) \\cdot y_{x-1} + (1 - \\alpha)^2 \\cdot \\hat{y}_{x-2} \\\\\n",
    "          & = & \\alpha \\cdot [ y_x + (1 - \\alpha) \\cdot y_{x-1} + (1 - \\alpha)^2 \\cdot y_{x-2} + \\dots + (1 - \\alpha)^{x-1} \\cdot y_1] + (1 - \\alpha)^{x} \\cdot y_0 \\\\\n",
    "\\end{eqnarray} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose alpha between 0 and 1. Lower values of alpha adapt to changes slower, so lead to more stable predictions, but don’t adapt so quickly to genuine change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def exponential_smoothing(series, alpha):\n",
    "    # given a series and alpha, return series of smoothed points\n",
    "    smoothed = [series[0]]\n",
    "    for i in range(1,len(series)):\n",
    "        smoothed.append(alpha * series[i] + (1 - alpha) * smoothed[i-1])\n",
    "    return smoothed\n",
    "\n",
    "series= [3, 9.3, 11.73, 12.87, 12.08, 10.20, 11.82, 12.89, 13.78, 14.65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(series)), series, 'or', alpha=0.5)\n",
    "plt.plot(np.arange(len(series)), exponential_smoothing(series, 0.7), 'g', alpha=0.5)\n",
    "plt.plot(np.arange(len(series)), exponential_smoothing(series, 0.9), 'b', alpha=0.5)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can measure the exponentially **weighted mean-squared-error** of previous predictions from the actual values. This gives an expected range of current deviation from predicted value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def std_exponential_smoothing(series, alpha, beta):\n",
    "    import numpy as np\n",
    "    std = np.zeros(len(series))\n",
    "    smoothed = np.zeros(len(series))\n",
    "    smoothed[0] = series[0]\n",
    "    for i in range(1,len(series)):\n",
    "        smoothed[i] = (alpha * series[i] + (1 - alpha) * smoothed[i-1])\n",
    "        std[i] = (1 - beta) * (std[i-1] + beta * (series[i] - smoothed[i-1])**2) \n",
    "    return np.sqrt(std) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "series= [3, 9.3, 11.73, 12.87, 12.08, 10.20, 11.82, 12.89, 13.78, 14.65]\n",
    "pred = exponential_smoothing(series, 0.5)\n",
    "std = std_exponential_smoothing(series, 0.5, 0.3)\n",
    "\n",
    "plt.plot(np.arange(len(series)), series, 'or', alpha=0.9)\n",
    "plt.plot(np.arange(len(series)), pred + std, 'g', alpha=0.5)\n",
    "plt.plot(np.arange(len(series)), pred - std, 'g', alpha=0.5)\n",
    "plt.plot(np.arange(len(series)), pred , 'b', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "series= [3, 9.3, 11.73, 12.87, 12.08, 10.20, 11.82, 12.89, 13.78, 14.65, 8.8, 15.0]\n",
    "pred = exponential_smoothing(series, 0.5)\n",
    "std = std_exponential_smoothing(series, 0.5, 0.05)\n",
    "plt.plot(np.arange(len(series)), series, 'or', alpha=0.9)\n",
    "plt.plot(np.arange(len(series)), pred + std, 'g', alpha=0.5)\n",
    "plt.plot(np.arange(len(series)), pred - std, 'g', alpha=0.5)\n",
    "plt.plot(np.arange(len(series)), pred , 'b', alpha=0.5)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
